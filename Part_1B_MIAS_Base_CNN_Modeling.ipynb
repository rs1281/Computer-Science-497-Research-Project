{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adefe82a",
   "metadata": {},
   "source": [
    "# Classifying Tumors in Mammograms\n",
    "\n",
    "# Part 1B - MIAS Base CNN Modeling\n",
    "\n",
    "The [MIAS dataset](http://peipa.essex.ac.uk/info/mias.html) was the initial dataset used to explore classifying images for tumor identification in mammograms. \n",
    "\n",
    "## Contents\n",
    "- [Data Import](#Data-Import-&-Cleaning)\n",
    "- [Prepare Images for Modeling](#Prepare-images-for-modeling)\n",
    "- [Modeling](#Modeling)\n",
    "    - [CNN](#CNN)\n",
    "    - [Other Examples of CNN Attempts](#Other-Examples-of-CNN-Attempts)\n",
    "    - [Bayes Search](#Bayes-Search-to-try-and-Optimize-the-CNN)\n",
    "- [Conclusions](#Conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1fe465",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Modeling Techniques Explored\n",
    "\n",
    "- Resizing\n",
    "- Gray scaling of image\n",
    "- Reflection - flip images horizontally and vertically\n",
    "- Rotation - Form of upsampling your dataset since it creates rotated copies of the same image.\n",
    "\n",
    "\n",
    "### Choice of Hyperparameters\n",
    "- Loss function - binary_crossentropy\n",
    "- Optimizer - Adam (Looking for a steady decrease in loss rate)\n",
    "- Metrics - Since we're dealing with life and death cancer diagnoses, False Negatives are far more detrimental since they mean you miss diiagnosing a tumor so reducing False Negatives is the aim. Also should pay attention to Accuracy and F1 score. Accuracy is important since we care how many True Positives our model finds but the F1 score is the harmonic mean of the model's Precision and Recall and is used when the False Negatives and False Positives might be more critical. In our classification problem, there is definitely imbalanced class distribution and thus F1-score might be a better metric to evaluate our model on.\n",
    "- Learning Rate / Epochs - Aimed for slower learning rates\n",
    "- The sigmoid function is used for the two-class logistic regression, whereas the softmax function is used for the multiclass logistic regression. In our case, we used the sigmoid for the tumor/no tumor classification. If we were to expand this project to tumor types we would need to use the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4757d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f9b066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6526a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5dcaf4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.core.framework.types_pb2' has no attribute 'SerializedDType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4fcbeb93e5c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0ml1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1_l2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m# Bring in subpackages.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# from tensorflow.python import keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAUTOTUNE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mservice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdense_to_ragged_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdense_to_sparse_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    417\u001b[0m \"\"\"\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfrom_dataset_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregister_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_service_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompression_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_server_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# ==============================================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;34m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgen_experimental_dataset_ops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mged_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwrapt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \"\"\"\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparse_tensor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sparse_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mop_callbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dtypes.DType\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DType\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m class DType(\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0m_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDType\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTraceType\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py\u001b[0m in \u001b[0;36mDType\u001b[1;34m()\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m   \u001b[1;32mdef\u001b[0m \u001b[0mexperimental_type_proto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mType\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtypes_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializedDType\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m     \u001b[1;34m\"\"\"Returns the type of proto associated with DType serialization.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtypes_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializedDType\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.core.framework.types_pb2' has no attribute 'SerializedDType'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import category_encoders as ce\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix,ConfusionMatrixDisplay\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7e52f3",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Files located in ./all-mias.tar/\n",
    "#images all start with mdb\n",
    "\n",
    "files = glob.glob('mias_data/all-mias/mdb*')\n",
    "#print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ffe1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the files\n",
    "#image_size sets the image dimensions, base is 1024x1024\n",
    "image_size = 128\n",
    "images = []\n",
    "#print(\"reached here\")\n",
    "for file in files:\n",
    "    #print(\"reached here 2\")\n",
    "    image = cv2.imread(file,0)\n",
    "    image = cv2.resize(image, (image_size, image_size))\n",
    "    images.append(image)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633971c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Display First ten images\n",
    "print (len(images))\n",
    "for i in range(10):\n",
    "    plt.imshow(images[i],cmap=plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6234c3",
   "metadata": {},
   "source": [
    "## Load in the documentation on tumors\n",
    "\n",
    "Text file provided with:\n",
    "MIAS database reference number.\n",
    "\n",
    " - Character of background tissue:\n",
    "\n",
    "        F  Fatty\n",
    "        G  Fatty-glandular\n",
    "        D  Dense-glandular\n",
    "     \n",
    "\n",
    " - Class of abnormality present:\n",
    "\n",
    "          CALC  Calcification\n",
    "          CIRC  Well-defined/circumscribed masses\n",
    "          SPIC  Spiculated masses\n",
    "          MISC  Other, ill-defined masses\n",
    "          ARCH  Architectural distortion\n",
    "          ASYM  Asymmetry\n",
    "          NORM  Normal\n",
    "\n",
    " - Severity of abnormality;\n",
    "\n",
    "          B  Benign\n",
    "          M  Malignant\n",
    "      \n",
    " - x,y image-coordinates of centre of abnormality.\n",
    "\n",
    " - Approximate radius (in pixels) of a circle enclosing the abnormality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14c4c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in CSV, cleaned separately of extraneous information\n",
    "labels = pd.read_csv('mias_data/all-mias/info_red.csv',delimiter=',',names=['ID','bg','ab_type','diagnosis','ab_x','ab_y','ab_r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b6c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e63b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If no tumor, make an N\n",
    "#Add tumor column\n",
    "labels['diagnosis'].fillna('N',inplace=True)\n",
    "labels['tumor'] = labels['diagnosis'].map(lambda x: 0 if x == 'N' else 1) # new list called tumor for 0 and 1's \n",
    "labels['tumor'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe2895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make mask layout\n",
    "labels['ab_x'].fillna(0,inplace=True)\n",
    "labels['ab_y'].fillna(0,inplace=True)\n",
    "labels['ab_r'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57469ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for the biggest and smallest box\n",
    "labels[labels['ab_r'] > 0]['ab_r'].max(),labels[labels['ab_r'] > 0]['ab_r'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that all empty things are filled\n",
    "labels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b09e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#330 entries, 322 images, some duplications\n",
    "labels['ID'] = labels['ID'].map(lambda x: x[3:])\n",
    "labels['ID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ba4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check breakdown of tumor diagnosis\n",
    "labels['diagnosis'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b4634",
   "metadata": {},
   "source": [
    "## Prepare images for modeling\n",
    "\n",
    " - Scale the images\n",
    " - Setup X and y\n",
    "    - Classify y two ways, either tumor or not, or type of tumor\n",
    "    - Loss function, final activation, and output layer size modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images[1])\n",
    "images = [image/255 for image in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a689aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ebce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce74ae49",
   "metadata": {},
   "source": [
    "Comment out one of the next two blocks, first block does classification on tumor type, next does it just on whether or not there is a tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba08ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.expand_dims(images,axis=3)\n",
    "# y = labels.drop_duplicates(subset='ID')[['ab_type']]\n",
    "# encoder = ce.OneHotEncoder(cols=['ab_type'], use_cat_names=True,\n",
    "#                         return_df=False)\n",
    "# print(y['ab_type'].nunique())\n",
    "# y = encoder.fit_transform(y)\n",
    "# loss_func, act_func, out_layer = 'categorical_crossentropy', 'softmax', 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9394705",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(images,axis=3)\n",
    "#print(\"x is this\")\n",
    "#print(X)\n",
    "y = labels.drop_duplicates(subset='ID')['tumor']\n",
    "loss_func, act_func, out_layer = 'binary_crossentropy', 'sigmoid', 1\n",
    "\n",
    "#print(y)\n",
    "y.shape\n",
    "#X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45504d0f",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa0fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 42)\n",
    "\n",
    "# \"train_test_split\" function to split your dataset\n",
    "# Takes in: \n",
    "    ##: input data and corresponding labels\n",
    "# Returns: \n",
    "    ##: multiple arrays or matrices representing the train-test splits\n",
    "    \n",
    "# X represents: input data\n",
    "# Y respresents: corresponding labels\n",
    "# random_state: ensures reproducibility by fixing the random seed\n",
    "\n",
    "# function returns four arrays: X_train, X_test, y_train, and y_test\n",
    "# data is split randomly, ensuring that both the training and testing subsets have a representative distribution of the original data\n",
    "\n",
    "# what is the point of doimg this? And splitting the data randomly? \n",
    "# By splitting the data into separate training and testing sets, you can assess the generalization ability of your model \n",
    "        #and estimate its performance on unseen data, \n",
    "        #helping you make informed decisions about the model's effectiveness and potential areas of improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df71490",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,y_train.shape,X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069e98ea",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Various sets of CNN fits were tried, with the following:\n",
    " - Number and size of hidden Convolution layers (1 to 4 layers, 10-128 filters)\n",
    " - Number and size Dense Layers (1-5 dense layers, 120-2000 nodes)\n",
    " - Various forms of regularization (L1,Batch, and Dropout)\n",
    " - Image size (100-1024 pixels)\n",
    " - Rotations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d1208f",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16289836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate model\n",
    "cnn = Sequential()\n",
    "\n",
    "# The Sequential class is used to create a sequential model, \n",
    "# where layers are stacked one after another to form the neural network architecture\n",
    "\n",
    "\n",
    "#convolution layers       -----------------------# defines the architecture of your neural network by adding layers in a sequential manner as shown below\n",
    "# -----------------\n",
    "# -----------------\n",
    "cnn.add(Conv2D(filters = 20,       #number of filters to use\n",
    "              kernel_size = (4,4),   #dimensions of the filters\n",
    "              activation = 'relu',    #activation function\n",
    "              input_shape = (image_size, image_size,1)))  #shape of image\n",
    "cnn.add(MaxPooling2D(pool_size = (2,2))) \n",
    "cnn.add(Conv2D(20, kernel_size = (4,4), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2,2)))\n",
    "cnn.add(Flatten())\n",
    "\n",
    "#dense layers\n",
    "# saw an example of this on Chat GPT \n",
    "# -------------\n",
    "# We are using 2 dense layers \n",
    "cnn.add(Dense(128, activation = 'relu'))   # the first layer has 128 units \n",
    "cnn.add(Dense(48, activation = 'relu'))    # the second layer has 48 units \n",
    "# The term \"units\" refers to the number of neurons in a particular layer of the network\n",
    "# Each unit or neuron represents a learnable parameter that processes input data and produces an output \n",
    "# The number of units in a layer has an impact on the complexity and capacity of the neural network. \n",
    "#Increasing the number of units generally increases the model's ability to capture complex patterns and representations in the data.\n",
    "\n",
    "\n",
    "# Example lines of the regularization methods tried\n",
    "# cnn.add(Dropout(0.2))\n",
    "# cnn.add(BatchNormalization())\n",
    "# ,kernel_regularizer=l2(0.01)\n",
    "# cnn.add(Dropout(0.3))\n",
    "\n",
    "#finalize things\n",
    "cnn.add(Dense(out_layer, activation = act_func))#  the Dense class represents a fully connected layer where each neuron is connected to every neuron in the previous layer.\n",
    "                                                # The out_layer parameter specifies the number of units (neurons) in this dense layer,\n",
    "                                                # act_func specifies the activation function to be used. The activation function adds non-linearity to the output of the neurons.\n",
    "    cnn.compile(loss = loss_func,               # The loss function measures the discrepancy between the predicted outputs and the true values.# \n",
    "           optimizer = 'adam',                  # The adam optimizer is a popular optimization algorithm used to update the weights of the neural network during the training process.\n",
    "           metrics = ['accuracy'])              # 'accuracy' metric is used, which calculates the accuracy of the model's predictions compared to the true labels.\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stop to end model, patience varied as needed when changin models\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "# validation loss (val_loss): The training process will monitor the validation loss and make decisions based on its behavior.\n",
    "# min_delta=0: This parameter defines the minimum change in the monitored metric that qualifies as an improvement. \n",
    "            # A value of 0 means any decrease in the metric will be considered an improvement. \n",
    "            # If the change in the monitored metric is smaller than min_delta, it will not be considered as an improvement.\n",
    "# The patience parameter determines the number of epochs to wait before stopping the training if no improvement is observed.\n",
    "# In this case, verbose=1 means the callback will provide detailed output.\n",
    "# 'auto' mode automatically determines the direction of improvement based on the metric being monitored. \n",
    "            # For example, if the monitored metric is loss, it will assume a lower value is better.\n",
    "\n",
    "    \n",
    "# By using the EarlyStopping callback, you can prevent overfitting and save computational resources by \n",
    "        # stopping the training process early when the monitored metric (in this case, validation loss) is no longer improving\n",
    "    # This helps in avoiding unnecessary training iterations and allows for efficient model training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b705515",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Perform the fit, epochs set to 1000 since early stop being used. # Not getting any better so stop before a 1000 iterations \n",
    "tf.random.set_seed(42)\n",
    "history_bw = cnn.fit(X_train, y_train, validation_data = (X_test, y_test),\n",
    "                 batch_size = 16,\n",
    "                 epochs = 1000,\n",
    "                 verbose = 1,\n",
    "                 callbacks = [early_stop]\n",
    ")\n",
    "\n",
    "# The cnn.fit method executes the training process, \n",
    "# where the model's weights are updated using the provided training data, loss function, and optimization algorithm. \n",
    "# The model's performance is monitored on the validation data to prevent overfitting \n",
    "# and determine when to stop training early if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40cfd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions, round since output is in probability\n",
    "preds = cnn.predict(X_test).round(0)\n",
    "#Base score\n",
    "metrics.f1_score(y_test,preds),metrics.accuracy_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa267fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "cm = confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4cd76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot CM\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecced93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for plotting CNN history\n",
    "def Loss_Acc_Plot(acc,val_acc,loss,val_loss):\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12,8))\n",
    "    fig.suptitle(\"Model Performance\", fontsize = 25, fontweight = 'bold')\n",
    "    fig.savefig('Accuracy_Loss_figure.png')\n",
    "\n",
    "    ax1.plot(range(1, len(acc) + 1), acc,  color='#185fad')\n",
    "    ax1.plot(range(1, len(val_acc) + 1), val_acc, color='orange')\n",
    "    ax1.set_title('History of Accuracy by Epoch', fontsize=20)\n",
    "    ax1.set_xlabel('Epoch', fontsize=18)\n",
    "    ax1.set_ylabel('Accuracy', fontsize=18)\n",
    "    ax1.legend(['training', 'validation'], fontsize=18)\n",
    "\n",
    "\n",
    "    ax2.plot(range(1, len(loss) + 1), loss, color='#185fad')\n",
    "    ax2.plot(range(1, len(val_loss) + 1), val_loss, color='orange')\n",
    "    ax2.set_title('History of Loss by Epoch', fontsize=20)\n",
    "    ax2.set_xlabel('Epoch', fontsize=18)\n",
    "    ax2.set_ylabel('Binary Crossentropy (Loss)', fontsize=18)\n",
    "    ax2.legend(['training', 'validation'], fontsize=18)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Understand this graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff077607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the plot for the history\n",
    "Loss_Acc_Plot(history_bw.history['accuracy'],history_bw.history['val_accuracy'],\n",
    "               history_bw.history['loss'],history_bw.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc980bd",
   "metadata": {},
   "source": [
    "## Other Examples of CNN Attempts\n",
    "\n",
    "The following shows some of the other example nets tried, rows were deactivated or modified to try different things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c505df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(42)\n",
    "# cnn = Sequential()\n",
    "# pool_size = 2\n",
    "# cnn.add(Conv2D(filters = 32,       #number of filters to use\n",
    "#               kernel_size = (4,4),   #dimensions of the filters\n",
    "#               activation = 'relu',    #activation function\n",
    "#               input_shape = (image_size,image_size,1), #shape of image\n",
    "#               padding='valid', \n",
    "#               #strides=(pool_size,pool_size)\n",
    "#               ))  \n",
    "# cnn.add(MaxPooling2D(pool_size = (pool_size,pool_size))) \n",
    "# cnn.add(Conv2D(32, kernel_size = (4,4), activation = 'relu'))\n",
    "# cnn.add(MaxPooling2D(pool_size = (pool_size,pool_size)))\n",
    "# # cnn.add(Conv2D(20, kernel_size = (4,4), activation = 'relu'))\n",
    "# # cnn.add(MaxPooling2D(pool_size = (pool_size,pool_size)))\n",
    "# cnn.add(Flatten())\n",
    "# # cnn.add(Dense(32, activation = 'relu'))\n",
    "# # cnn.add(Dense(32, activation = 'relu'))\n",
    "# # cnn.add(Dense(32, activation = 'relu'))\n",
    "# # cnn.add(Dense(32, activation = 'relu'))\n",
    "# # cnn.add(Dense(32, activation = 'relu'))\n",
    "# cnn.add(Dropout(0))\n",
    "# cnn.add(Dense(1024, activation = 'relu'))\n",
    "# cnn.add(Dropout(0.2))\n",
    "# # cnn.add(Dense(32, activation = 'relu'))\n",
    "\n",
    "# cnn.summary()\n",
    "# cnn.add(Dense(out_layer, activation = act_func))\n",
    "# cnn.compile(loss = loss_func,\n",
    "#            optimizer = 'adam',\n",
    "#            metrics = ['accuracy'])\n",
    "# # history_bw = cnn.fit(X_train, y_train, validation_data = (X_test, y_test),\n",
    "# #                  batch_size = 32,\n",
    "# #                  epochs = 1000,\n",
    "# #                  verbose = 1,\n",
    "# #                  callbacks = [early_stop]\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4838d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn = Sequential()\n",
    "# cnn.add(Conv2D(filters = 20,       #number of filters to use\n",
    "#               kernel_size = (4,4),   #dimensions of the filters\n",
    "#               activation = 'relu',    #activation function\n",
    "#               input_shape = (image_size,image_size,1)))  #shape of image\n",
    "# cnn.add(MaxPooling2D(pool_size = (2, 2))) \n",
    "# cnn.add(Conv2D(20, kernel_size = (4,4), activation = 'relu'))\n",
    "# cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "# cnn.add(Flatten())\n",
    "# cnn.add(Dense(2048, activation = 'relu'))\n",
    "# #cnn.add(Dense(512, activation = 'relu'))\n",
    "# #cnn.add(Dense(32, activation = 'relu'))\n",
    "# cnn.add(Dense(1, activation = 'sigmoid'))\n",
    "# cnn.compile(loss = 'binary_crossentropy',\n",
    "#            optimizer = 'adam',\n",
    "#            metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn = Sequential()\n",
    "\n",
    "# cnn.add(Conv2D(32, (3,3), padding='valid', strides=(1, 1),input_shape = (image_size,image_size,1), activation = 'relu'))\n",
    "# cnn.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "# cnn.add(Conv2D(64, (3,3), padding='same', strides=(1, 1), activation = 'relu'))\n",
    "# cnn.add(MaxPooling2D(pool_size=(3, 3),strides=2))\n",
    "# # cnn.add(Dropout(0.2))\n",
    "\n",
    "# # cnn.add(Conv2D(64, (5,5), padding='same', strides=(1, 1), activation = 'relu'))\n",
    "# # cnn.add(MaxPooling2D(pool_size=(3, 3),strides=2))\n",
    "\n",
    "# # cnn.add(Conv2D(32, (5,5), padding='same', strides=(1, 1), activation = 'relu'))\n",
    "# # cnn.add(MaxPooling2D(pool_size=(2, 2),strides=2))\n",
    "\n",
    "# # cnn.add(Dropout(0.2))\n",
    "# cnn.add(Flatten())\n",
    "# cnn.add(Dense(128, activation = 'relu'))\n",
    "# cnn.add(Dense(64, activation = 'relu'))\n",
    "# cnn.add(Dense(out_layer, activation = act_func))\n",
    "# cnn.compile(loss = loss_func,\n",
    "#            optimizer = 'adam',\n",
    "#            metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f546c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn = Sequential()\n",
    "\n",
    "# cnn.add(Conv2D(filters = 32,       #number of filters to use\n",
    "#               kernel_size = (3, 3),   #dimensions of the filters\n",
    "#               activation = 'relu',    #activation function\n",
    "#               input_shape = (128,128,1), #shape of image\n",
    "#               #padding='valid', \n",
    "#               #strides=(1, 1)\n",
    "#               ))  \n",
    "# cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# cnn.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "# cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# cnn.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "# cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# #cnn.add(Dropout(0.25))\n",
    "# cnn.add(Flatten())\n",
    "# cnn.add(Dense(96, activation = 'relu'))\n",
    "# cnn.add(Dense(1, activation = 'sigmoid'))\n",
    "# cnn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "# early_stop = EarlyStopping(monitor = 'val_loss', \n",
    "#                            min_delta = 0,\n",
    "#                            patience = 5, \n",
    "#                            verbose = 1, \n",
    "#                            mode='auto', \n",
    "#                            restore_best_weights=True)\n",
    "# # Fit model on training data\n",
    "# history_bw = cnn.fit(X_train, y_train, validation_data = (X_test, y_test),\n",
    "#                  batch_size = 64,\n",
    "#                  epochs = 100,\n",
    "#                  verbose = 1,\n",
    "#                  callbacks=[early_stop]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc21c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn = Sequential()\n",
    "\n",
    "# cnn.add(Conv2D(filters = 64,       #number of filters to use\n",
    "#               kernel_size = (3, 3),   #dimensions of the filters\n",
    "#               activation = 'relu',    #activation function\n",
    "#               input_shape = (128,128,1), #shape of image\n",
    "#               #padding='valid', \n",
    "#               #strides=(1, 1)\n",
    "#               ))  \n",
    "# cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# cnn.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "# cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# cnn.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "# cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# cnn.add(Dropout(0.25))\n",
    "# cnn.add(Flatten())\n",
    "# cnn.add(Dense(96, activation = 'relu'))\n",
    "# cnn.add(Dense(64, activation = 'relu'))\n",
    "# cnn.add(Dense(64, activation = 'relu'))\n",
    "# cnn.add(Dense(1, activation = 'sigmoid'))\n",
    "# cnn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "# early_stop = EarlyStopping(monitor = 'val_loss', \n",
    "#                            min_delta = 0,\n",
    "#                            patience = 5, \n",
    "#                            verbose = 1, \n",
    "#                            mode='auto', \n",
    "#                            restore_best_weights=True)\n",
    "# # Fit model on training data\n",
    "# history_bw = cnn.fit(X_train, y_train, validation_data = (X_test, y_test),\n",
    "#                  batch_size = 64,\n",
    "#                  epochs = 100,\n",
    "#                  verbose = 1,\n",
    "#                  callbacks=[early_stop]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797db04a",
   "metadata": {},
   "source": [
    "## Bayes Search to try and Optimize the CNN\n",
    "\n",
    "Attempted a Bayes search with several levels of varying many of the parameters. The results did not turn out well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create a model for the Bayes search\n",
    "\n",
    "# def model_func(first_fil, sec_fil, kernel_1,kernel_2,pool_1,pool_2,layer_1,layer_2,layer_3,dropout):\n",
    "#     cnn = Sequential()\n",
    "#     cnn.add(Conv2D(filters = first_fil,       #number of filters to use\n",
    "#                   kernel_size = (kernel_1,kernel_1),   #dimensions of the filters\n",
    "#                   activation = 'relu',    #activation function\n",
    "#                   input_shape = (image_size, image_size,1)))  #shape of image\n",
    "#     cnn.add(MaxPooling2D(pool_size = (pool_1, pool_1))) \n",
    "#     cnn.add(Conv2D(sec_fil, kernel_size = (kernel_2, kernel_2), activation = 'relu'))\n",
    "#     cnn.add(MaxPooling2D(pool_size = (pool_2, pool_2)))\n",
    "#     cnn.add(Flatten())\n",
    "#     cnn.add(Dense(layer_1, activation = 'relu'))\n",
    "#     cnn.add(Dropout(dropout))\n",
    "#     cnn.add(Dense(layer_2, activation = 'relu'))\n",
    "#     cnn.add(Dropout(dropout))\n",
    "#     cnn.add(Dense(layer_3, activation = 'relu'))\n",
    "#     cnn.add(Dropout(dropout))\n",
    "#     cnn.add(Dense(1, activation = 'sigmoid'))\n",
    "#     cnn.compile(loss = 'binary_crossentropy',\n",
    "#                optimizer = 'adam',\n",
    "#                metrics = ['accuracy'])\n",
    "#     return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf8d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create classifier pipeline\n",
    "# nn = KerasClassifier(build_fn = model_func,batch_size = 32, epochs = 12,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7451a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes Search parameter grid\n",
    "\n",
    "# params = {'first_fil':Integer(4,64),\n",
    "#          'sec_fil':Integer(4,64),\n",
    "#          'kernel_1':Integer(2,10),\n",
    "#          'kernel_2':Integer(2,10),\n",
    "#          'pool_1':Integer(2,5),\n",
    "#          'pool_2':Integer(2,5),\n",
    "#          'layer_1':Integer(10,64),\n",
    "#          'layer_2':Integer(10,64),\n",
    "#          'layer_3':Integer(10,64),\n",
    "#          'dropout':Real(0.01,0.3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d93b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the Bayes search\n",
    "\n",
    "# cnn_bs = BayesSearchCV(estimator = nn,\n",
    "#                      search_spaces = params,\n",
    "#                      n_iter = 200,\n",
    "#                      cv = 3,\n",
    "#                      verbose = 1,\n",
    "#                      random_state=42)\n",
    "# cnn_bs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6596df4e",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "- Massively overfit models and performance is not much better than baseline 62.7%\n",
    "- Need more training data and to explore more image preprocessing and data augmentation methods\n",
    "- Be sure to Train test split before augmenting any images. When you rotate images it creates copies and if you split these after the fact you will most likely cause data leakage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
